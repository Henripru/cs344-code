{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Perceptrons and XOR\n",
    "\n",
    "Perceptrons, part of work in cybernetics, showed early promise but failed to produce a learning algorithm capable of learning the XOR function. This is because XOR is not *linearly separable*.\n",
    "\n",
    "We demonstrate these ideas using linear and non-linear models built using TensorFlow/Keras rather than using the original perceptron formulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class will implement a variety of simple MLP models for logic gates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicGateModel:\n",
    "    '''This class models a two-input logic gate function, i.e., AND, OR, XOR, NAND, NOR, XNOR\n",
    "    hidden_layers gives a list of layer specifications of the form: (#ofNodes, activationFunction);\n",
    "    output_layer is similar but with only layer specification.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 optimizer=SGD(lr=0.1),\n",
    "                 hidden_layers=[],\n",
    "                 output_layer=(1, tf.keras.activations.linear),\n",
    "                 loss_function=tf.keras.losses.mse\n",
    "                 ):\n",
    "        self.model = Sequential()\n",
    "        for layer in hidden_layers:\n",
    "            self.model.add(Dense(layer[0], input_dim=2, activation=layer[1]))\n",
    "        self.model.add(Dense(output_layer[0], input_dim=2, activation=output_layer[1]))\n",
    "        self.model.compile(loss=loss_function,\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=[tf.keras.metrics.binary_accuracy]\n",
    "                           )\n",
    "\n",
    "    def train(self, X, y, n_epochs=100):\n",
    "        self.model.fit(X, y, batch_size=1, epochs=n_epochs, verbose=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define AND, OR and XOR logic functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answers\n",
      "         AND OR XOR\n",
      "[0 0] ->  0  0   0\n",
      "[0 1] ->  0  1   1\n",
      "[1 0] ->  0  1   1\n",
      "[1 1] ->  1  1   0\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y_and = np.array([[0],[0],[0],[1]])\n",
    "y_or = np.array([[0],[1],[1],[1]])\n",
    "y_xor = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Print the correct answers.\n",
    "print('Correct answers')\n",
    "print(\"         AND OR XOR\")\n",
    "for i in range(4):\n",
    "    print(f'{X[i]} ->  '\n",
    "          f'{X[i,0] & X[i,1]}  '\n",
    "          f'{X[i,0] | X[i,1]}   '\n",
    "          f'{X[i,0] ^ X[i,1]}'\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AND function can be learned by a one-layer network with a linear activation function. See how the output for [1,1] is clearly the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.27583656]\n",
      " [ 0.24837926]\n",
      " [ 0.25031343]\n",
      " [ 0.7745292 ]]\n"
     ]
    }
   ],
   "source": [
    "el = LogicGateModel(output_layer=(1, tf.keras.activations.linear))\n",
    "model.train(X, y_and)\n",
    "print(momoddel.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same holds for the OR function. See how the output value for [0.0] is clearly lower than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2983376 ]\n",
      " [0.7861295 ]\n",
      " [0.80061376]\n",
      " [1.2884055 ]]\n"
     ]
    }
   ],
   "source": [
    "model = LogicGateModel(output_layer=(1, tf.keras.activations.linear))\n",
    "model.train(X, y_or)\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, a linear model cannot learn the XOR function, regardless of how many layers we add. See how the outputs fail to converge on the correct solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.44690713]\n",
      " [0.37721825]\n",
      " [0.36193404]\n",
      " [0.29224515]]\n",
      "[[0.4463554 ]\n",
      " [0.44695613]\n",
      " [0.44801366]\n",
      " [0.4486144 ]]\n",
      "[[0.5807274 ]\n",
      " [0.58397406]\n",
      " [0.5806303 ]\n",
      " [0.58387697]]\n"
     ]
    }
   ],
   "source": [
    "# One layer\n",
    "model = LogicGateModel(output_layer=(1, tf.keras.activations.linear))\n",
    "model.train(X, y_xor)\n",
    "print(model.predict(X))\n",
    "\n",
    "# Two layers\n",
    "model = LogicGateModel(hidden_layers=[(2, tf.keras.activations.linear)])\n",
    "model.train(X, y_xor)\n",
    "print(model.predict(X))\n",
    "\n",
    "# More layers\n",
    "model = LogicGateModel(hidden_layers=[\n",
    "    (4, tf.keras.activations.linear),\n",
    "    (2, tf.keras.activations.linear)\n",
    "])\n",
    "model.train(X, y_xor)\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A non-linear regression model can learn the XOR function. Here, a non-linear tanh() is used. This model, suggested by Goodfellow, et al., does not always find the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00]\n",
      " [9.9999988e-01]\n",
      " [9.9999988e-01]\n",
      " [1.7881393e-07]]\n"
     ]
    }
   ],
   "source": [
    "model = LogicGateModel(\n",
    "    hidden_layers=[(2, tf.nn.tanh)],\n",
    "    output_layer=(1, tf.keras.activations.linear)\n",
    ")\n",
    "model.train(X, y_xor, n_epochs=500)\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This slightly more complicated model, suggested by S. Park, works more reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00249194]\n",
      " [0.9953402 ]\n",
      " [0.99494237]\n",
      " [0.00601137]]\n"
     ]
    }
   ],
   "source": [
    "model = LogicGateModel(\n",
    "    hidden_layers=[(8, tf.nn.tanh)],\n",
    "    output_layer=(1, tf.nn.sigmoid),\n",
    "    loss_function=tf.keras.losses.binary_crossentropy\n",
    ")\n",
    "model.train(X, y_xor, n_epochs=1000)\n",
    "print(model.predict(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
