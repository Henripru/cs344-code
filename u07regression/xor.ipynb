{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Perceptrons and XOR\n",
    "\n",
    "Perceptrons, part of work in cybernetics, showed early promise but failed to produce a learning algorithm capable of learning the XOR function. This is because XOR is not *linearly separable*.\n",
    "\n",
    "We demonstrate these ideas using linear and non-linear models built using TensorFlow/Keras rather than using the original perceptron formulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class will implement a variety of simple MLP models for logic gates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicGateModel:\n",
    "    '''This class models a two-input logic gate function, i.e., AND, OR, XOR, NAND, NOR, XNOR\n",
    "    hidden_layers gives a list of layer specifications of the form: (#ofNodes, activationFunction);\n",
    "    output_layer is similar but with only layer specification.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 optimizer=SGD(lr=0.1),\n",
    "                 hidden_layers=[],\n",
    "                 output_layer=(1, tf.keras.activations.linear),\n",
    "                 loss_function=tf.keras.losses.mse\n",
    "                 ):\n",
    "        self.model = Sequential()\n",
    "        for layer in hidden_layers:\n",
    "            self.model.add(Dense(layer[0], input_dim=2, activation=layer[1]))\n",
    "        self.model.add(Dense(output_layer[0], input_dim=2, activation=output_layer[1]))\n",
    "        self.model.compile(loss=loss_function,\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=[tf.keras.metrics.binary_accuracy]\n",
    "                           )\n",
    "\n",
    "    def train(self, X, y, n_epochs=100):\n",
    "        self.model.fit(X, y, batch_size=1, epochs=n_epochs, verbose=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define AND, OR and XOR logic functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answers\n",
      "         AND OR XOR\n",
      "[0 0] ->   0   0    0\n",
      "[0 1] ->   0   1    1\n",
      "[1 0] ->   0   1    1\n",
      "[1 1] ->   1   1    0\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y_and = np.array([[0],[0],[0],[1]])\n",
    "y_or = np.array([[0],[1],[1],[1]])\n",
    "y_xor = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Print the correct answers.\n",
    "print('Correct answers')\n",
    "print(\"         AND OR XOR\")\n",
    "for i in range(4):\n",
    "    print('{} ->  '.format(X[i]),\n",
    "          '{}  '.format(X[i,0] & X[i,1]),\n",
    "          '{}   '.format(X[i,0] | X[i,1]),\n",
    "          '{}'.format(X[i,0] ^ X[i,1])\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AND function can be learned by a one-layer network with a linear activation function. See how the output for [1,1] is clearly the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.23015495]\n",
      " [ 0.304143  ]\n",
      " [ 0.33643454]\n",
      " [ 0.8707325 ]]\n"
     ]
    }
   ],
   "source": [
    "model = LogicGateModel(output_layer=(1, tf.keras.activations.linear))\n",
    "model.train(X, y_and)\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same holds for the OR function. See how the output value for [0.0] is clearly lower than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.31762263]\n",
      " [0.81649065]\n",
      " [0.81242156]\n",
      " [1.3112895 ]]\n"
     ]
    }
   ],
   "source": [
    "model = LogicGateModel(output_layer=(1, tf.keras.activations.linear))\n",
    "model.train(X, y_or)\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, a linear model cannot learn the XOR function, regardless of how many layers we add. See how the outputs fail to converge on the correct solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47008473]\n",
      " [0.356476  ]\n",
      " [0.40504622]\n",
      " [0.2914375 ]]\n",
      "[[0.49500963]\n",
      " [0.49542087]\n",
      " [0.47822714]\n",
      " [0.47863835]]\n",
      "[[0.5077244 ]\n",
      " [0.49466753]\n",
      " [0.51906437]\n",
      " [0.5060075 ]]\n"
     ]
    }
   ],
   "source": [
    "# One layer\n",
    "model = LogicGateModel(output_layer=(1, tf.keras.activations.linear))\n",
    "model.train(X, y_xor)\n",
    "print(model.predict(X))\n",
    "\n",
    "# Two layers\n",
    "model = LogicGateModel(hidden_layers=[(2, tf.keras.activations.linear)])\n",
    "model.train(X, y_xor)\n",
    "print(model.predict(X))\n",
    "\n",
    "# More layers\n",
    "model = LogicGateModel(hidden_layers=[\n",
    "    (4, tf.keras.activations.linear),\n",
    "    (2, tf.keras.activations.linear)\n",
    "])\n",
    "model.train(X, y_xor)\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A non-linear regression model can learn the XOR function. Here, a non-linear tanh() is used. This model, suggested by Goodfellow, et al., does not always find the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.3841858e-07]\n",
      " [9.9999976e-01]\n",
      " [9.9999988e-01]\n",
      " [0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "model = LogicGateModel(\n",
    "    hidden_layers=[(2, tf.nn.tanh)],\n",
    "    output_layer=(1, tf.keras.activations.linear)\n",
    ")\n",
    "model.train(X, y_xor, n_epochs=500)\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This slightly more complicated model, suggested by S. Park, works more reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00219102]\n",
      " [0.99306774]\n",
      " [0.9937768 ]\n",
      " [0.00844336]]\n"
     ]
    }
   ],
   "source": [
    "model = LogicGateModel(\n",
    "    hidden_layers=[(8, tf.nn.tanh)],\n",
    "    output_layer=(1, tf.nn.sigmoid),\n",
    "    loss_function=tf.keras.losses.binary_crossentropy\n",
    ")\n",
    "model.train(X, y_xor, n_epochs=1000)\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
