{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "Artificial Neural Networks (ANN) are inspired by the network structure of the human brain.\n",
    "They are built from individual artificial neurons, which are mathematical models of neurons \n",
    "that can be viewed as follows.\n",
    "\n",
    "![artificial neuron](https://cs.calvin.edu/courses/cs/344/kvlinden/07regression/images/an.png)\n",
    "\n",
    "Notes:\n",
    "- The neuron receives *activation* from other input neurons ($a_i$).\n",
    "- A *bias* input can be added to each neuron ($a_0$).\n",
    "- Each input activation is weighted (i.e., the weight for input $i$ to neuron $j$ is $w_{ij}$).\n",
    "- The output activation is computed for each neuron ($a_j = g(in_j) = g(\\sum_{i=0}^n w_{ij} \\cdot a_i$)).\n",
    "\n",
    "Artificial neurons can be configured into feed-forward networks comprising multiple-layers. \n",
    "Here is an example.\n",
    "\n",
    "![artificial neural network](https://cs.calvin.edu/courses/cs/344/kvlinden/07regression/images/ann.png)\n",
    "\n",
    "We train these ANNs using the backpropagation algorithm:\n",
    "\n",
    "**1. Initialization**\n",
    "\n",
    "Set the ANN weights to \"small\", \"random\" numbers.\n",
    "\n",
    "**2. Feed-Forward**\n",
    "\n",
    "Take an example input from the training set and feed its activation through the network.\n",
    "\n",
    "$\n",
    "o_j = \n",
    "\\left[\n",
    "\\begin{array}{c c}\n",
    "i_1 & i_2 \\\\ \n",
    "\\end{array}\n",
    "\\right]\n",
    "\\cdot\n",
    "\\left[\n",
    "\\begin{array}{c c}\n",
    "w_{i_1,h_1} & w_{i_1,h_2} \\\\ \n",
    "w_{i_2,h_1} & w_{i_2,h_2} \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\cdot\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "w_{h_1, o_1} \\\\ \n",
    "w_{h_2, o_1} \\\\ \n",
    "\\end{array}\n",
    "\\right]\n",
    "$\n",
    "\n",
    "**3. Error Computation**\n",
    "\n",
    "Compute the error using a common error function (e.g., L2 error) by comparing the desired output ($y_j$) with the actual output ($o_j$).\n",
    "\n",
    "$\n",
    "L2\\_Error = \\sum_{i=1}^n {(y_j - o_j)}^2\n",
    "$\n",
    "\n",
    "**4. Backpropagation** \n",
    "\n",
    "Modify the weights by propagating updates back through the network using a given learning rate ($rate$) and the raw errors generated by the output layer ($\\Delta_{o_j} = y_j - o_j$).\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "w_{i_i,h_j}^* &\\leftarrow w_{i_i,h_j} + rate \\cdot a_{i_i} \\cdot g'(in_{h_j}) \\cdot \\sum_k (w_{h_i, o_k} \\cdot \\Delta_{o_k}) \\\\\n",
    "w_{h_j,o_k}^* &\\leftarrow w_{h_j,o_k} + rate \\cdot a_{h_j} \\cdot g'(in_{o_k}) \\cdot \\Delta_{o_k}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "These backpropagation formulae are derived by computing the following, the first for the hidden layer(s) (more complicated) and the second for the output layer (less complicated).\n",
    "\n",
    "$\n",
    "{\\partial{Error_o} \\over {\\partial{W_{i,h}}}} = \\ldots = -a_i \\cdot g'(in_{h_j}) \\cdot \\sum_k (w_{h_i, o_k} \\cdot \\Delta_{o_k}) \\\\\n",
    "{\\partial{Error_o} \\over {\\partial{W_{h,o}}}} = \\ldots = -a_h \\cdot g'(in_{o_k}) \\cdot \\Delta_{o_k}\n",
    "$\n",
    "\n",
    "Interestingly, deep neural networks appear to work in practice without significant problems caused by local minima."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
