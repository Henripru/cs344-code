{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "source": "# Artificial Neural Networks\n\nArtificial Neural Networks (ANN) are inspired by the network structure of the human brain.\nThey are built from individual artificial neurons, which are mathematical models of neurons \nthat can be viewed as follows.\n\n\u003cimg src\u003d\"https://cs.calvin.edu/courses/cs/344/kvlinden/07regression/images/an.png\" width\u003d\"256px\"/\u003e\n\nNotes:\n- The neuron receives *activation* from other input neurons ($a_i$).\n- A *bias* input can be added to each neuron ($a_0$).\n- Each input activation is weighted (i.e., the weight for input $i$ to neuron $j$ is $w_{ij}$).\n- The output activation is computed for each neuron ($a_j \u003d g(in_j) \u003d g(\\sum_{i\u003d0}^n w_{ij} \\cdot a_i$)).\n\nArtificial neurons can be configured into feed-forward networks comprising multiple-layers. \nHere is an example.\n\n\u003cimg src\u003d\"https://cs.calvin.edu/courses/cs/344/kvlinden/07regression/images/ann.png\" width\u003d\"256px\"/\u003e\n\nNotes:\n- This network architecture is:\n    - *Feed-forward*: The activation flows from the input (top) to the output (bottom).\n    - *Fully/Densely inter-connected*: The output of each node is fed into all nodes in the next layer.\n- Layers in such networks can be seen as non-linear functions of the outputs of the preceeding layers, i.e.: \n    $o \u003d layer_h(layer_i(i))$\n\nWe train these ANNs using the backpropagation algorithm:\n\n**1. Initialization**\n\nSet the ANN weights to \"small\", \"random\" numbers.\n\n**2. Feed-Forward**\n\nTake an example input from the training set and feed its activation through the network.\n\n$\no_1 \u003d \n\\left[\n\\begin{array}{c c}\ni_1 \u0026 i_2 \\\\ \n\\end{array}\n\\right]\n\\cdot\n\\left[\n\\begin{array}{c c}\nw_{i_1,h_1} \u0026 w_{i_1,h_2} \\\\ \nw_{i_2,h_1} \u0026 w_{i_2,h_2} \\\\\n\\end{array}\n\\right]\n\\cdot\n\\left[\n\\begin{array}{c}\nw_{h_1, o_1} \\\\ \nw_{h_2, o_1} \\\\ \n\\end{array}\n\\right]\n$\n\n**3. Error Computation**\n\nCompute the error using a common error function (e.g., L2 error) by comparing the desired output ($y_j$) with the actual output ($o_j$).\n\n$\nL_2Error \u003d \\sum_{i\u003d1}^n {(y_j - o_j)}^2 \n$\n\n**4. Backpropagation** \n\nModify the weights by propagating updates back through the network using a given learning rate ($rate$) and the raw errors generated by the output layer ($\\Delta_{o_j} \u003d y_j - o_j$).\n\n$\n\\begin{aligned}\nW_{i,h}^\\ast \u0026\\leftarrow W_{i,h} + rate \\cdot A_{i} \\cdot g\u0027(in_{h}) \\odot \\sum_k (W_{h, o} \\cdot \\Delta_{o}) \\\\\nW_{h,o}^\\ast \u0026\\leftarrow W_{h,o} + rate \\cdot A_{h} \\cdot g\u0027(in_{o}) \\cdot \\Delta_{o}\n\\end{aligned}\n$\n\nThese backpropagation formulae are derived by computing the following, the first for the hidden layer(s) (more complicated) and the second for the output layer (less complicated).\n\n$\n\\begin{aligned}\n{{\\partial{Error_o}} \\over {\\partial{W_{i,h}}}} \u0026\u003d \\ldots \u003d -A_i \\cdot g\u0027(in_{h}) \\odot \\sum_k (W_{h, o} \\cdot \\Delta_{o_k}) \\\\\n{{\\partial{Error_o}} \\over {\\partial{W_{h,o}}}} \u0026\u003d \\ldots \u003d -A_h \\cdot g\u0027(in_{o}) \\cdot \\Delta_{o}\n\\end{aligned}\n$\n\nInterestingly, deep neural networks appear to work in practice without significant problems caused by local minima."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "## Example\n\nIn class, we ran the following example (inspired by [Backpropagation Step by Step](https://hmkcode.github.io/ai/backpropagation-step-by-step/)).\n\n1. Fill in random weights.\n\n    $\\begin{aligned}\n    \u0026\\begin{bmatrix}\n    w_{i_1,h_1} \u0026 w_{i_1,h_2} \\\\\n    w_{i_2,h_1} \u0026 w_{i_2,h_2}\n    \\end{bmatrix}\n    \\leftarrow\n    \\begin{bmatrix}\n    0.11 \u0026 0.12 \\\\\n    0.21 \u0026 0.08\n    \\end{bmatrix} \\\\\n    \u0026\\begin{bmatrix}\n    w_{h_1, o_1} \\\\ \n    w_{h_2, o_1} \n    \\end{bmatrix}\n    \\leftarrow\n    \\begin{bmatrix}\n    0.14 \\\\\n    0.15\n    \\end{bmatrix}\n    \\end{aligned}$\n    \n2. Compute the output for one sample (XOR: `[0, 1]` \u0026rarr; `1`).\n\n    $\\begin{aligned}\n    o_j \u0026\u003d \n    \\begin{bmatrix}\n    0 \u0026 1 \\\\ \n    \\end{bmatrix}\n    \\cdot\n    \\begin{bmatrix}\n    0.11 \u0026 0.12 \\\\\n    0.21 \u0026 0.08\n    \\end{bmatrix}\n    \\cdot\n    \\begin{bmatrix}\n    0.14 \\\\\n    0.15\n    \\end{bmatrix}\n    \\\\ \u0026\u003d\n    \\begin{bmatrix}\n    0 * 0.11 + 1 * 0.21 \u0026 0 * 0.12 + 1 * 0.08\n    \\end{bmatrix}\n    \\cdot\n    \\begin{bmatrix}\n    0.14 \\\\ \n    0.15\n    \\end{bmatrix}\n    \\\\ \u0026\u003d\n    \\begin{bmatrix}\n    0.21 \u0026 0.08\n    \\end{bmatrix}\n    \\cdot\n    \\begin{bmatrix}\n    0.14 \\\\ \n    0.15 \n    \\end{bmatrix}\n    \\\\ \u0026\u003d\n    \\begin{bmatrix}\n    0.21 * 0.14 + 0.08 * 0.15\n    \\end{bmatrix}\n    \\\\ \u0026\u003d 0.0414 \n    \\end{aligned}\n    \\\\\n    $\n\n3. Compute the error (and more importantly, the delta).\n\n    $\\begin{aligned}\n    L_2Error \u0026\u003d (1 - 0.0414)^2 \\\\\n    \u0026\u003d 0.9189 \\\\\n    \\Delta_{o_1} \u0026\u003d (1 - 0.0414) \\\\\n    \u0026\u003d 0.9586 \\\\\n    \\end{aligned}$\n\n4. Backpropagate updates back through the network, assuming: \n    $learning\\_rate \u003d 0.05$; \n    RELU activation functions for all nodes.\n     \n    $\\begin{aligned}\n    \\begin{bmatrix}\n    w_{h_1, o_1} \\\\ \n    w_{h_2, o_1}\n    \\end{bmatrix} \u0026\\leftarrow \n    \\begin{bmatrix}\n    0.14 \\\\ \n    0.15 \n    \\end{bmatrix} + 0.05 \\cdot \n    \\begin{bmatrix}\n    0.21 \\\\ \n    0.08 \n    \\end{bmatrix} \\cdot 1.0 \\cdot 0.9586 \\\\\\\\\n    \u0026\u003d \n    \\begin{bmatrix}\n    0.14 \\\\ \n    0.15 \n    \\end{bmatrix} + \n    \\begin{bmatrix}\n    0.05 * 0.21 * 1.0 * 0.9586 \\\\\n    0.05 * 0.08 * 1.0 * 0.9586 \n    \\end{bmatrix} \\\\\n    \u0026\u003d \n    \\begin{bmatrix}\n    0.14 \\\\ \n    0.15 \n    \\end{bmatrix} +\n    \\begin{bmatrix}\n    0.0100 \\\\\n    0.00383 \n    \\end{bmatrix} \\\\\n    \u0026\u003d\n    \\begin{bmatrix}\n    0.1500 \\\\ \n    0.1538\n    \\end{bmatrix}\n    \\end{aligned}$\n\n    $\\begin{aligned}\n    \\begin{bmatrix}\n    w_{i_1,h_1} \u0026 w_{i_1,h_2} \\\\ \n    w_{i_2,h_1} \u0026 w_{i_2,h_2}\n    \\end{bmatrix} \u0026\\leftarrow \n    \\begin{bmatrix}\n    0.11 \u0026 0.12 \\\\\n    0.21 \u0026 0.08\n    \\end{bmatrix} + 0.05 \\cdot\n    \\begin{bmatrix}\n    0 \u0026 0 \\\\ \n    1 \u0026 1\n    \\end{bmatrix} \\cdot 1.0 \\odot\n    \\begin{bmatrix}\n    0.14 \u0026 0.15 \\\\ \n    0.14 \u0026 0.15\n    \\end{bmatrix} \\cdot 0.9586 \\\\ \u0026\u003d\n    \\begin{bmatrix}\n    0.11 \u0026 0.12 \\\\\n    0.21 \u0026 0.08\n    \\end{bmatrix} + \n    \\begin{bmatrix}\n    0.05 * 0 * 1.0 \u0026 0.05 * 0 * 1.0 \\\\ \n    0.05 * 1 * 1.0 \u0026 0.05 * 1 * 1.0 \\\\ \n    \\end{bmatrix} \\odot \n    \\begin{bmatrix}\n    0.14 * 0.9586 \u0026 0.15 * 0.9586\\\\ \n    0.14 * 0.9586 \u0026 0.15 * 0.9586 \n    \\end{bmatrix} \\\\ \u0026\u003d\n    \\begin{bmatrix}\n    0.11 \u0026 0.12 \\\\\n    0.21 \u0026 0.08\n    \\end{bmatrix} + \n    \\begin{bmatrix}\n    0.00 \u0026 0.00 \\\\ \n    0.05 \u0026 0.05 \n    \\end{bmatrix} \\odot \n    \\begin{bmatrix}\n    0.1342 \u0026 0.1438 \\\\\n    0.1342 \u0026 0.1438\n    \\end{bmatrix} \\\\ \u0026\u003d\n    \\begin{bmatrix}\n    0.11 \u0026 0.12 \\\\\n    0.21 \u0026 0.08\n    \\end{bmatrix} + \n    \\begin{bmatrix}\n    0.00 * 0.1342 \u0026 0.00 * 0.1438 \\\\\n    0.05 * 0.1342 \u0026 0.05 * 0.1438\n    \\end{bmatrix} \\\\ \u0026\u003d\n    \\begin{bmatrix}\n    0.11 \u0026 0.12 \\\\\n    0.21 \u0026 0.08\n    \\end{bmatrix} +     \n    \\begin{bmatrix}\n    0.0 \u0026 0.0 \\\\\n    0.0067 \u0026 0.0072\n    \\end{bmatrix} \\\\ \u0026\u003d \n    \\begin{bmatrix}\n    0.11 \u0026 0.12 \\\\\n    0.2167 \u0026 0.0872\n    \\end{bmatrix}  \n    \\end{aligned}$\n\nNotes:\n- We had to do 2 *broadcasts* (in the first line) to get the \n    required matrix dimensions. \n    - $A_i$: along the vertical (i.e., add duplicate column).\n    - $W_{h,o}$ along the horizontal (i.e., add duplicate row).\n    - This allows us to use element-wise multiplication, known as the [Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) and denoted by $\\odot$.\n- This process:\n    - works recursively for multiple layered networks.\n    - is more efficient that adjusting each weight individually.\n    - is generally run using *Mini-batch Stochastic Gradient Descent*.             "
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}